# README

- zdobycie niezbędnej wiedzy redukuje zapotrzebowanie na nowe próbki
- nasycenie wiedzą można mierzyć tym czteroliterowym skrótem

'RBFGradualRecurring', 'RBFNoDrift', 'RBFBlips', 'SEASuddenFaster',
  'SEASudden', 'LEDNoDrift', 'LED', 'HyperplaneFaster', 'HyperplaneSlow',
  'elecNormNew', 'covtypeNorm', 'poker-lsn'

![](figures/RBFGradualRecurring.png)
![](figures/RBFNoDrift.png)
![](figures/RBFBlips.png)
![](figures/SEASuddenFaster.png)
![](figures/SEASudden.png)
![](figures/LEDNoDrift.png)
![](figures/LED.png)
![](figures/HyperplaneFaster.png)
![](figures/HyperplaneSlow.png)
![](figures/elecNormNew.png)
![](figures/covtypeNorm.png)
![](figures/poker-lsn.png)

<!--
Pozytywny wpływ na jakość klasyfikacji (a również na dynamikę krzywej uczenia) mają zarówno liczba neuronów w warstwie ukrytej (im więcej tym lepiej), jak i liczba próbek znajdujących się w pojedynczym chunku (im mniej tym lepiej).

Bujda: Zmniejszenie wielkości chunka wpływa też negatywnie na stabilność wyniku.
-->
